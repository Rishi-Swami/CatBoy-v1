# ğŸ¾ CatBoy GPT - Minimal GPT-Style Chatbot 

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
![Status](https://img.shields.io/badge/status-active-success.svg)
![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)

A compact, educational implementation of a **transformer-based causal language model (mini-GPT)** built entirely from scratch in **PyTorch**.  
Created for learning, experimentation, and a deeper understanding of how GPT-style models work.

---

## âœ¨ Features

- ğŸ§  **Custom Regex Tokenizer** with save/load support  
- âš™ï¸ **Transformer Blocks** with causal masking  
- ğŸš€ **Training Script** (CPU/GPU compatible)  
- ğŸ’¬ **Interactive Chat Program** (`catboy.py`)  
- ğŸ² **Text Generation** with temperature & top-k sampling  

---

## âš¡ Quick Start

### 1ï¸âƒ£ Install requirements

    pip install -r requirements.txt

### 2ï¸âƒ£ Prepare dataset

Place your training text file in:

    dataset/training_data.txt

Each line should contain **one training example**.

### 3ï¸âƒ£ Train the model

    python train.py

> ğŸ’¡ Automatically uses GPU if available; otherwise defaults to CPU.

### 4ï¸âƒ£ Chat interactively

    python catboy.py

### 5ï¸âƒ£ Generate text from CLI

    python generate.py --prompt "Hello" --max_new_tokens 30

---

## ğŸ§© Project Structure

    CatBoy/
    â”œâ”€â”€ dataset/
    â”‚   â””â”€â”€ training_data.txt
    â”œâ”€â”€ saved_model_learning/   # auto-created, gitignored
    â”œâ”€â”€ tokenizer.py
    â”œâ”€â”€ model.py
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ train.py
    â”œâ”€â”€ generate.py
    â”œâ”€â”€ catboy.py
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt

---

## ğŸ§  Notes & Implementation Details

- `ignore_index` used in loss to skip learning padding tokens.  
- Transformer block includes **causal masking** and proper transposition for `nn.MultiheadAttention`.  
- Tokenizer supports `<PAD>` and `<UNK>` tokens, plus **save/load** to avoid train/infer mismatches.  
- Generation supports **temperature** and **top-k sampling**:
  - `temperature=0.9` - more creative  
  - `temperature=0.1` - more deterministic  

---

## âš™ï¸ Requirements

    torch
    tqdm

> ğŸ’¡ On CPU-only systems: if you encounter CUDA errors, install the CPU-only PyTorch wheel as described in the official PyTorch docs: https://pytorch.org/get-started/locally/

---

## ğŸ§¼ Suggested `.gitignore`

    __pycache__/
    *.pyc
    saved_model_learning/
    *.pth
    tokenizer.json

---

## ğŸª¶ License

This project is open-source and available under the **MIT License**.  
Youâ€™re free to use, modify, and distribute it with attribution.

---

## ğŸ’¬ Final Notes

CatBoy GPT is designed as a **learning-focused, minimal GPT implementation** which is ideal for understanding the inner workings of modern transformer-based language models.

For larger-scale or production-grade models, consider:
CatBoy-v2 or CatBoy-v3
---

### Meow responsibly. Keep coding curiously.
